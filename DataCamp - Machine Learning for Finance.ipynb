{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataCamp - Machine Learning for Finance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore data with some EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's explore the data. Any time we begin a machine learning (ML) project, we need to first do some exploratory data analysis (EDA) to familiarize ourselves with the data. This includes things like:\n",
    "\n",
    "raw data plots\n",
    "histograms\n",
    "and more...\n",
    "I typically begin with raw data plots and histograms. This allows us to understand our data's distributions. If it's a normal distribution, we can use things like parametric statistics.\n",
    "\n",
    "There are two stocks loaded for you into pandas DataFrames: lng_df and spy_df (LNG and SPY). Take a look at them with .head(). We'll use the closing prices and eventually volume as inputs to ML algorithms.\n",
    "\n",
    "Note: We'll call plt.clf() each time we want to make a new plot, or f = plt.figure()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (<ipython-input-1-8e79ebd0bd76>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-8e79ebd0bd76>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    lng_df[____].plot(label=____, ____, secondary_y=True)\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "print(lng_df.head())  # examine the DataFrames\n",
    "print(spy_df.head())  # examine the SPY DataFrame\n",
    "\n",
    "# Plot the Adj_Close columns for SPY and LNG\n",
    "spy_df['Adj_Close'].plot(label='SPY', legend=True)\n",
    "lng_df['Adj_Close'].plot(label='LNG', secondary_y=True)\n",
    "plt.show()  # show the plot\n",
    "plt.clf()  # clear the plot space\n",
    "\n",
    "# Histogram of the daily price change percent of Adj_Close for LNG\n",
    "lng_df['Adj_Close'].pct_change(1).plot.hist(bins=50)\n",
    "plt.xlabel('adjusted close 1-day percent change')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlations are nice to check out before building machine learning models, because we can see which features correlate to the target most strongly. Pearson's correlation coefficient is often used, which only detects linear relationships. It's commonly assumed our data is normally distributed, which we can \"eyeball\" from histograms. Highly correlated variables have a Pearson correlation coefficient near 1 (positively correlated) or -1 (negatively correlated). A value near 0 means the two variables are not linearly correlated.\n",
    "\n",
    "If we use the same time periods for previous price changes and future price changes, we can see if the stock price is mean-reverting (bounces around) or trend-following (goes up if it has been going up recently)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 5-day % changes of Adj_Close for the current day, and 5 days in the future\n",
    "lng_df['5d_future_close'] = lng_df['Adj_Close'].shift(-5)\n",
    "lng_df['5d_close_future_pct'] = lng_df['5d_future_close'].pct_change(5)\n",
    "lng_df['5d_close_pct'] = lng_df['Adj_Close'].pct_change(5)\n",
    "\n",
    "# Calculate the correlation matrix between the 5d close pecentage changes (current and future)\n",
    "corr = lng_df[['5d_close_pct', '5d_close_future_pct']].corr()\n",
    "print(corr)\n",
    "\n",
    "# Scatter the current 5-day percent change vs the future 5-day percent change\n",
    "plt.scatter(lng_df['5d_close_pct'], lng_df['5d_close_future_pct'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create moving average and RSI features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to add historical data to our machine learning models to make better predictions, but adding lots of historical time steps is tricky. Instead, we can condense information from previous points into a single timestep with indicators.\n",
    "\n",
    "A moving average is one of the simplest indicators - it's the average of previous data points. This is the function talib.SMA() from the TAlib library.\n",
    "\n",
    "Another common technical indicator is the relative strength index (RSI). This is defined by:\n",
    "\n",
    "RSI=100âˆ’1001+RS \n",
    "\n",
    "RS=average gain over n periods / average loss over n periods\n",
    "\n",
    "The n periods is set in talib.RSI() as the timeperiod argument.\n",
    "\n",
    "A common period for RSI is 14, so we'll use that as one setting in our calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['5d_close_pct']  # a list of the feature names for later\n",
    "\n",
    "# Create moving averages and rsi for timeperiods of 14, 30, 50, and 200\n",
    "for n in [14, 30, 50, 200]:\n",
    "\n",
    "    # Create the moving average indicator and divide by Adj_Close\n",
    "    lng_df['ma' + str(n)] = talib.SMA(lng_df['Adj_Close'].values,\n",
    "                              timeperiod=n) / lng_df['Adj_Close']\n",
    "    # Create the RSI indicator\n",
    "    lng_df['rsi' + str(n)] = talib.RSI(lng_df['Adj_Close'].values, timeperiod=n)\n",
    "    \n",
    "    # Add rsi and moving average to the feature name list\n",
    "    feature_names = feature_names + ['ma' + str(n), 'rsi' + str(n)]\n",
    "\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create features and targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We almost have features and targets that are machine-learning ready -- we have features from current price changes (5d_close_pct) and indicators (moving averages and RSI), and we created targets of future price changes (5d_close_future_pct). Now we need to break these up into separate numpy arrays so we can feed them into machine learning algorithms.\n",
    "\n",
    "Our indicators also cause us to have missing values at the beginning of the DataFrame due to the calculations. We could backfill this data, fill it with a single value, or drop the rows. Dropping the rows is a good choice, so our machine learning algorithms aren't confused by any sort of backfilled or 0-filled data. Pandas has a .dropna() function which we will use to drop any rows with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all na values\n",
    "lng_df = lng_df.dropna()\n",
    "\n",
    "# Create features and targets\n",
    "# use feature_names for features; 5d_close_future_pct for targets\n",
    "features = lng_df[feature_names]\n",
    "targets = lng_df['5d_close_future_pct']\n",
    "\n",
    "# Create DataFrame from target column and feature columns\n",
    "feat_targ_df = lng_df[['5d_close_future_pct'] + feature_names]\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr = feat_targ_df.corr()\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we fit our first machine learning model, let's look at the correlations between features and targets. Ideally we want large (near 1 or -1) correlations between features and targets. Examining correlations can help us tweak features to maximize correlation (for example, altering the timeperiod argument in the talib functions). It can also help us remove features that aren't correlated to the target.\n",
    "\n",
    "To easily plot a correlation matrix, we can use seaborn's heatmap() function. This takes a correlation matrix as the first argument, and has many other options. Check out the annot option -- this will help us turn on annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot heatmap of correlation matrix\n",
    "sns.heatmap(corr, annot=True)\n",
    "plt.yticks(rotation=0); plt.xticks(rotation=90)  # fix ticklabel directions\n",
    "plt.tight_layout()  # fits plot area to the plot, \"tightly\"\n",
    "plt.show()  # show the plot\n",
    "plt.clf() # clear the plot area\n",
    "\n",
    "# Create a scatter plot of the most highly correlated variable with the target\n",
    "plt.scatter(x=lng_df['ma200'], y=lng_df['5d_close_future_pct'] )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train and test features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we fit our linear model, we want to add a constant to our features, so we have an intercept for our linear model.\n",
    "\n",
    "We also want to create train and test features. This is so we can fit our model to the train dataset, and evaluate performance on the test dataset. We always want to check performance on data the model has not seen to make sure we're not overfitting, which is memorizing patterns in the training data too exactly.\n",
    "\n",
    "With a time series like this, we typically want to use the oldest data as our training set, and the newest data as our test set. This is so we can evaluate the performance of the model on the most recent data, which will more realistically simulate predictions on data we haven't seen yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the statsmodels.api library with the alias sm\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Add a constant to the features\n",
    "linear_features = sm.add_constant(features)\n",
    "\n",
    "# Create a size for the training set that is 85% of the total number of samples\n",
    "train_size = int(0.85 * targets.shape[0])\n",
    "train_features = linear_features[:train_size]\n",
    "train_targets = targets[:train_size]\n",
    "test_features = linear_features[train_size:]\n",
    "test_targets = targets[train_size:]\n",
    "print(linear_features.shape, train_features.shape, test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now fit a linear model, because they are simple and easy to understand. Once we've fit our model, we can see which predictor variables appear to be meaningfully linearly correlated with the target, as well as their magnitude of effect on the target. Our judgment of whether or not predictors are significant is based on the p-values of coefficients. This is using a t-test to statistically test if the coefficient significantly differs from 0. The p-value is the percent chance that the coefficient for a feature does not differ from zero. Typically, we take a p-value of less than 0.05 to mean the coefficient is significantly different from 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the linear model and complete the least squares fit\n",
    "model = sm.OLS(train_targets, train_features)\n",
    "results = model.fit()  # fit the model\n",
    "print(results.summary())\n",
    "\n",
    "# examine pvalues\n",
    "# Features with p <= 0.05 are typically considered significantly different from 0\n",
    "print(results.pvalues)\n",
    "\n",
    "# Make predictions from our model for train and test sets\n",
    "train_predictions = results.predict(train_features)\n",
    "test_predictions = results.predict(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate our results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our linear fit and predictions, we want to see how good the predictions are so we can decide if our model is any good or not. Ideally, we want to back-test any type of trading strategy. However, this is a complex and typically time-consuming experience.\n",
    "\n",
    "A quicker way to understand the performance of our model is looking at regression evaluation metrics like R2, and plotting the predictions versus the actual values of the targets. Perfect predictions would form a straight, diagonal line in such a plot, making it easy for us to eyeball how our predictions are doing in different regions of price changes. We can use matplotlib's .scatter() function to create scatter plots of the predictions and actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter the predictions vs the targets with 80% transparency\n",
    "plt.scatter(train_predictions, train_targets, alpha=0.2, color='b', label='train')\n",
    "plt.scatter(test_predictions, test_targets, alpha=0.2, color='r', label='test')\n",
    "\n",
    "# Plot the perfect prediction line\n",
    "xmin, xmax = plt.xlim()\n",
    "plt.plot(np.arange(xmin, xmax, 0.01), np.arange(xmin, xmax, 0.01), c='k')\n",
    "\n",
    "# Set the axis labels and show the plot\n",
    "plt.xlabel('predictions')\n",
    "plt.ylabel('actual')\n",
    "plt.legend()  # show the legend\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering from volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use non-linear models to make more accurate predictions. With linear models, features must be linearly correlated to the target. Other machine learning models can combine features in non-linear ways. For example, what if the price goes up when the moving average of price is going up, and the moving average of volume is going down? The only way to capture those interactions is to either multiply the features, or to use a machine learning algorithm that can handle non-linearity (e.g. random forests).\n",
    "\n",
    "To incorporate more information that may interact with other features, we can add in weakly-correlated features. First we will add volume data, which we have in the lng_df as the Adj_Volume column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2 new volume features, 1-day % change and 5-day SMA of the % change\n",
    "new_features = ['Adj_Volume_1d_change', 'Adj_Volume_1d_change_SMA']\n",
    "feature_names.extend(new_features)\n",
    "lng_df['Adj_Volume_1d_change'] = lng_df['Adj_Volume'].pct_change()\n",
    "lng_df['Adj_Volume_1d_change_SMA'] = talib.SMA(lng_df['Adj_Volume_1d_change'].values,timeperiod=5)\n",
    "\n",
    "# Plot histogram of volume % change data\n",
    "lng_df[new_features].plot(kind='hist', sharex=False, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create day-of-week features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can engineer datetime features to add even more information for our non-linear models. Most financial data has datetimes, which have lots of information in them -- year, month, day, and sometimes hour, minute, and second. But we can also get the day of the week, and things like the quarter of the year, or the elapsed time since some event (e.g. earnings reports).\n",
    "\n",
    "We are only going to get the day of the week here, since our dataset doesn't go back very far in time. The dayofweek property from the pandas datetime index will help us get the day of the week. Then we will dummy dayofweek with pandas' get_dummies(). This creates columns for each day of the week with binary values (0 or 1). We drop the first column because it can be inferred from the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas' get_dummies function to get dummies for day of the week\n",
    "days_of_week = pd.get_dummies(lng_df.index.dayofweek,\n",
    "                              prefix='weekday',\n",
    "                              drop_first=True)\n",
    "\n",
    "# Set the index as the original dataframe index for merging\n",
    "days_of_week.index = lng_df.index\n",
    "\n",
    "# Join the dataframe with the days of week dataframe\n",
    "lng_df = pd.concat([lng_df, days_of_week], axis=1)\n",
    "\n",
    "# Add days of week to feature names\n",
    "feature_names.extend(['weekday_' + str(i) for i in range(1, 5)])\n",
    "lng_df.dropna(inplace=True)  # drop missing values in-place\n",
    "print(lng_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine correlations of the new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our volume and datetime features, we want to check the correlations between our new features (stored in the new_features list) and the target (5d_close_future_pct) to see how strongly they are related. Recall pandas has the built-in .corr() method for DataFrames, and seaborn has a nice heatmap() function to show the correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the weekday labels to the new_features list\n",
    "new_features.extend(['weekday_' + str(i) for i in range(1, 5)])\n",
    "\n",
    "# Plot the correlations between the new features and the targets\n",
    "sns.heatmap(lng_df[new_features + ['5d_close_future_pct']].corr(), annot=True)\n",
    "plt.yticks(rotation=0)  # ensure y-axis ticklabels are horizontal\n",
    "plt.xticks(rotation=90)  # ensure x-axis ticklabels are vertical\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests are a go-to model for predictions; they work well out of the box. But we'll first learn the building block of random forests -- decision trees.\n",
    "\n",
    "Decision trees split the data into groups based on the features. Decision trees start with a root node, and split the data down until we reach leaf nodes.\n",
    "\n",
    "decision tree\n",
    "\n",
    "We can use sklearn to fit a decision tree with DecisionTreeRegressor and .fit(features, targets).\n",
    "\n",
    "Without limiting the tree's depth (or height), it will keep splitting the data until each leaf has 1 sample in it, which is the epitome of overfitting. We'll learn more about overfitting in the coming chapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Create a decision tree regression model with default arguments\n",
    "decision_tree = DecisionTreeRegressor()\n",
    "\n",
    "# Fit the model to the training features and targets\n",
    "decision_tree.fit(train_features, train_targets)\n",
    "\n",
    "# Check the score on train and test\n",
    "print(decision_tree.score(train_features, train_targets))\n",
    "print(decision_tree.score(test_features, test_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try different max depths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We always want to optimize our machine learning models to make the best predictions possible. We can do this by tuning hyperparameters, which are settings for our models. We will see in more detail how these are useful in future chapters, but for now think of them as knobs we can turn to tune our predictions to be as good as possible.\n",
    "\n",
    "For regular decision trees, probably the most important hyperparameter is max_depth. This limits the number of splits in a decision tree. Let's find the best value of max_depth based on the R2 score of our model on the test set, which we can obtain using the score() method of our decision tree models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through a few different max depths and check the performance\n",
    "for d in [3, 5, 10]:\n",
    "    # Create the tree and fit it\n",
    "    decision_tree = DecisionTreeRegressor(max_depth=d)\n",
    "    decision_tree.fit(train_features, train_targets)\n",
    "\n",
    "    # Print out the scores on train and test\n",
    "    print('max_depth=', str(d))\n",
    "    print(decision_tree.score(train_features, train_targets))\n",
    "    print(decision_tree.score(test_features, test_targets), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check our results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have an optimized model, we want to check how it is performing in more detail. We already saw the R2 score, but it can be helpful to see the predictions plotted vs actual values. We can use the .predict() method of our decision tree model to get predictions on the train and test sets.\n",
    "\n",
    "Ideally, we want to see diagonal lines from the lower left to the upper right. However, due to the simplicity of decisions trees, our model is not going to do well on the test set. But it will do well on the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best max_depth of 3 from last exercise to fit a decision tree\n",
    "decision_tree = DecisionTreeRegressor(max_depth = 3)\n",
    "decision_tree.fit(train_features, train_targets)\n",
    "\n",
    "# Predict values for train and test\n",
    "train_predictions = decision_tree.predict(train_features)\n",
    "test_predictions = decision_tree.predict(test_features)\n",
    "\n",
    "# Scatter the predictions vs actual values\n",
    "plt.scatter(train_predictions, train_targets, label='train')\n",
    "plt.scatter(test_predictions, test_targets, label='test')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data scientists often use random forest models. They perform well out of the box, and have lots of settings to optimize performance. Random forests can be used for classification or regression; we'll use it for regression to predict the future price change of LNG.\n",
    "\n",
    "We'll create and fit the random forest model similarly to the decision trees using the .fit(features, targets) method. With sklearn's RandomForestRegressor, there's a built-in .score() method we can use to evaluate performance. This takes arguments (features, targets), and returns the R2 score (the coefficient of determination)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Create the random forest model and fit to the training data\n",
    "rfr = RandomForestRegressor(n_estimators=200)\n",
    "rfr.fit(train_features, train_targets)\n",
    "\n",
    "# Look at the R^2 scores on train and test\n",
    "print(rfr.score(train_features, train_targets))\n",
    "print(rfr.score(test_features, test_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune random forest hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with all models, we want to optimize performance by tuning hyperparameters. We have many hyperparameters for random forests, but the most important is often the number of features we sample at each split, or max_features in RandomForestRegressor from the sklearn library. For models like random forests that have randomness built-in, we also want to set the random_state. This is set for our results to be reproducible.\n",
    "\n",
    "Usually, we can use sklearn's GridSearchCV() method to search hyperparameters, but with a financial time series, we don't want to do cross-validation due to data mixing. We want to fit our models on the oldest data and evaluate on the newest data. So we'll use sklearn's ParameterGrid to create combinations of hyperparameters to search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f2eb620c13da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Create a dictionary of hyperparameters to search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'n_estimators'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'max_depth'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'max_features'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'random_state'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# Create a dictionary of hyperparameters to search\n",
    "grid = {'n_estimators': [200], 'max_depth': [3], 'max_features': [4, 8], 'random_state': [42]}\n",
    "test_scores = []\n",
    "\n",
    "# Loop through the parameter grid, set the hyperparameters, and save the scores\n",
    "for g in ParameterGrid(grid):\n",
    "    rfr.set_params(**g)  # ** is \"unpacking\" the dictionary\n",
    "    rfr.fit(train_features, train_targets)\n",
    "    test_scores.append(rfr.score(test_features, test_targets))\n",
    "\n",
    "# Find best hyperparameters from the test score and print\n",
    "best_idx = np.argmax(test_scores)\n",
    "print(test_scores[best_idx], ParameterGrid(grid)[best_idx])\n",
    "\n",
    "# 0.048188439095540936 {'n_estimators': 200, 'max_depth': 3, 'random_state': 42, 'max_features': 4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, and as always, we want to evaluate performance of our best model to check how well or poorly we are doing. Ideally it's best to do back-testing, but that's an involved process we don't have room to cover in this course.\n",
    "\n",
    "We've already seen the R2 scores, but let's take a look at the scatter plot of predictions vs actual results using matplotlib. Perfect predictions would be a diagonal line from the lower left to the upper right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best hyperparameters from before to fit a random forest model\n",
    "rfr = RandomForestRegressor(n_estimators=200, max_depth=3, max_features=4, random_state=42)\n",
    "rfr.fit(train_features, train_targets)\n",
    "\n",
    "# Make predictions with our model\n",
    "train_predictions = rfr.predict(train_features)\n",
    "test_predictions = rfr.predict(test_features)\n",
    "\n",
    "# Create a scatter plot with train and test actual vs predictions\n",
    "plt.scatter(train_targets, train_predictions, label='train')\n",
    "plt.scatter(test_targets, test_predictions, label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest feature importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One useful aspect of tree-based methods is the ability to extract feature importances. This is a quantitative way to measure how much each feature contributes to our predictions. It can help us focus on our best features, possibly enhancing or tuning them, and can also help us get rid of useless features that may be cluttering up our model.\n",
    "\n",
    "Tree models in sklearn have a .feature_importances_ property that's accessible after fitting the model. This stores the feature importance scores. We need to get the indices of the sorted feature importances using np.argsort() in order to make a nice-looking bar plot of feature importances (sorted from greatest to least importance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rfr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-90caa98b9088>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get feature importances from our random forest model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimportances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrfr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Get the index of importances from greatest importance to least\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msorted_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimportances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rfr' is not defined"
     ]
    }
   ],
   "source": [
    "# Get feature importances from our random forest model\n",
    "importances = rfr.feature_importances_\n",
    "\n",
    "# Get the index of importances from greatest importance to least\n",
    "sorted_index = np.argsort(importances)[::-1]\n",
    "x = range(len(importances))\n",
    "\n",
    "# Create tick labels \n",
    "labels = np.array(feature_names)[sorted_index]\n",
    "plt.bar(x, importances[sorted_index], tick_label=labels)\n",
    "\n",
    "# Rotate tick labels to vertical\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A gradient boosting model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll fit a gradient boosting (GB) model. It's been said a linear model is like a Toyota Camry, and GB is like a Black Hawk helicopter. GB has potential to outperform random forests, but doesn't always do so. This is called the **no free lunch theorem**, meaning we should always try lots of different models for each problem.\n",
    "\n",
    "GB is similar to random forest models, but the difference is that trees are built successively. With each iteration, the next tree fits the residual errors from the previous tree in order to improve the fit.\n",
    "\n",
    "For now we won't search our hyperparameters -- they've been searched for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Create GB model -- hyperparameters have already been searched for you\n",
    "gbr = GradientBoostingRegressor(max_features=4,\n",
    "                                learning_rate=0.01,\n",
    "                                n_estimators=200,\n",
    "                                subsample=0.6,\n",
    "                                random_state=42)\n",
    "gbr.fit(train_features, train_targets)\n",
    "\n",
    "print(gbr.score(train_features, train_targets))\n",
    "print(gbr.score(test_features, test_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient boosting feature importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with random forests, we can extract feature importances from gradient boosting models to understand which features are the best predictors. Sometimes it's nice to try different tree-based models and look at the feature importances from all of them. This can help average out any peculiarities that may arise from one particular model.\n",
    "\n",
    "The feature importances are stored as a numpy array in the .feature_importances_ property of the gradient boosting model. We'll need to get the sorted indices of the feature importances, using np.argsort(), in order to make a nice plot. We want the features from largest to smallest, so we will use Python's indexing to reverse the sorted importances like feat_importances[::-1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importances from the fitted gradient boosting model\n",
    "feature_importances = gbr.feature_importances_\n",
    "\n",
    "# Get the indices of the largest to smallest feature importances\n",
    "sorted_index = np.argsort(feature_importances)[::-1]\n",
    "x = range(features.shape[1])\n",
    "\n",
    "# Create tick labels \n",
    "labels = np.array(feature_names)[sorted_index]\n",
    "\n",
    "plt.bar(x, feature_importances[sorted_index], tick_label=labels)\n",
    "\n",
    "# Set the tick lables to be the feature names, according to the sorted feature_idx\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some models, like K-nearest neighbors (KNN) & neural networks, work better with scaled data -- so we'll standardize our data.\n",
    "\n",
    "We'll also remove unimportant variables (day of week), according to feature importances, by indexing the features DataFrames with .iloc[]. KNN uses distances to find similar points for predictions, so big features outweigh small ones. Scaling data fixes that.\n",
    "\n",
    "sklearn's scale() will standardize data, which sets the mean to 0 and standard deviation to 1. Ideally we'd want to use StandardScaler with fit_transform() on the training data, and fit() on the test data, but we are limited to 15 lines of code here.\n",
    "\n",
    "Once we've scaled the data, we'll check that it worked by plotting histograms of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "\n",
    "# Remove unimportant features (weekdays)\n",
    "train_features = train_features.iloc[:, :-4]\n",
    "test_features = test_features.iloc[:, :-4]\n",
    "\n",
    "# Standardize the train and test features\n",
    "scaled_train_features = scale(train_features)\n",
    "scaled_test_features = scale(test_features)\n",
    "\n",
    "# Plot histograms of the 14-day SMA RSI before and after scaling\n",
    "f, ax = plt.subplots(nrows=2, ncols=1)\n",
    "train_features.iloc[:, 2].hist(ax=ax[0])\n",
    "ax[1].hist(scaled_train_features[:, 2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize n_neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have scaled data, we can try using a KNN model. To maximize performance, we should tune our model's hyperparameters. For the k-nearest neighbors algorithm, we only have one hyperparameter: n, the number of neighbors. We set this hyperparameter when we create the model with KNeighborsRegressor. The argument for the number of neighbors is n_neighbors.\n",
    "\n",
    "We want to try a range of values that passes through the setting with the best performance. Usually we will start with 2 neighbors, and increase until our scoring metric starts to decrease. We'll use the R2 value from the .score() method on the test set (scaled_test_features and test_targets) to optimize n here. We'll use the test set scores to determine the best n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "for n in range(2, 13):\n",
    "    # Create and fit the KNN model\n",
    "    knn = KNeighborsRegressor(n_neighbors=n)\n",
    "    \n",
    "    # Fit the model to the training data\n",
    "    knn.fit(scaled_train_features, train_targets)\n",
    "    \n",
    "    # Print number of neighbors and the score to find the best value of n\n",
    "    print(\"n_neighbors =\", n)\n",
    "    print('train, test scores')\n",
    "    print(knn.score(scaled_train_features, train_targets))\n",
    "    print(knn.score(scaled_test_features, test_targets))\n",
    "    print()  # prints a blank line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate KNN performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just saw a few things with our KNN scores. For one, the training scores started high and decreased with increasing n, which is typical. The test set performance reached a peak at 5 though, and we will use that as our setting in the final KNN model.\n",
    "\n",
    "As we have done a few times now, we will check our performance visually. This helps us see how well the model is predicting on different regions of actual values. We will get predictions from our knn model using the .predict() method on our scaled features. Then we'll use matplotlib's plt.scatter() to create a scatter plot of actual versus predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model with the best-performing n_neighbors of 5\n",
    "knn = KNeighborsRegressor(n_neighbors=5)\n",
    "\n",
    "# Fit the model\n",
    "knn.fit(scaled_train_features, train_targets)\n",
    "\n",
    "# Get predictions for train and test sets\n",
    "train_predictions = knn.predict(scaled_train_features)\n",
    "test_predictions = knn.predict(scaled_test_features)\n",
    "\n",
    "# Plot the actual vs predicted values\n",
    "plt.scatter(train_predictions, train_targets, label='train')\n",
    "plt.scatter(test_predictions, test_targets, label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and fit a simple neural net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next model we will learn how to use is a neural network. Neural nets can capture complex interactions between variables, but are difficult to set up and understand. Recently, they have been beating human experts in many fields, including image recognition and gaming (check out AlphaGo) -- so they have great potential to perform well.\n",
    "\n",
    "To build our nets we'll use the keras library. This is a high-level API that allows us to quickly make neural nets, yet still exercise a lot of control over the design. The first thing we'll do is create almost the simplest net possible -- a 3-layer net that takes our inputs and predicts a single value. Much like the sklearn models, keras models have a .fit() method that takes arguments of (features, targets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Create the model\n",
    "model_1 = Sequential()\n",
    "model_1.add(Dense(100, input_dim=scaled_train_features.shape[1], activation='relu'))\n",
    "model_1.add(Dense(20, activation='relu'))\n",
    "model_1.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Fit the model\n",
    "model_1.compile(optimizer='adam', loss='mse')\n",
    "history = model_1.fit(scaled_train_features, train_targets, epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've fit a model, we usually check the training loss curve to make sure it's flattened out. The history returned from model.fit() is a dictionary that has an entry, 'loss', which is the training loss. We want to ensure this has more or less flattened out at the end of our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the losses from the fit\n",
    "plt.plot(history.history['loss'])\n",
    "\n",
    "# Use the last loss as the title\n",
    "plt.title('loss:' + str(round(history.history['loss'][-1], 6)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've fit our neural net, let's check performance to see how well our model is predicting new values. There's not a built-in .score() method like with sklearn models, so we'll use the r2_score() function from sklearn.metrics. This calculates the R2 score given arguments (y_true, y_predicted). We'll also plot our predictions versus actual values again. This will yield some interesting results soon (once we implement our own custom loss function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Calculate R^2 score\n",
    "train_preds = model_1.predict(scaled_train_features)\n",
    "test_preds = model_1.predict(scaled_test_features)\n",
    "print(r2_score(train_targets, train_preds))\n",
    "print(r2_score(test_targets, test_preds))\n",
    "\n",
    "# Plot predictions vs actual\n",
    "plt.scatter(train_preds, train_targets, label='train')\n",
    "plt.scatter(test_preds, test_targets, label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to now, we've used the mean squared error as a loss function. This works fine, but with stock price prediction it can be useful to implement a custom loss function. A custom loss function can help improve our model's performance in specific ways we choose. For example, we're going to create a custom loss function with a large penalty for predicting price movements in the wrong direction. This will help our net learn to at least predict price movements in the correct direction.\n",
    "\n",
    "To do this, we need to write a function that takes arguments of (y_true, y_predicted). We'll also use functionality from the backend keras (using tensorflow) to find cases where the true value and prediction don't match signs, then penalize those cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.losses\n",
    "import tensorflow as tf\n",
    "\n",
    "# Create loss function\n",
    "def sign_penalty(y_true, y_pred):\n",
    "    penalty = 100.\n",
    "    loss = tf.where(tf.less(y_true * y_pred, 0), \\\n",
    "                     penalty * tf.square(y_true - y_pred), \\\n",
    "                     tf.square(y_true - y_pred))\n",
    "\n",
    "    return tf.reduce_mean(loss, axis=-1)\n",
    "\n",
    "keras.losses.sign_penalty = sign_penalty  # enable use of loss with keras\n",
    "print(keras.losses.sign_penalty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit neural net with custom loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use the custom loss function we just created. This will enable us to alter the model's behavior in useful ways particular to our problem -- it's going to try to force the model to learn how to at least predict price movement direction correctly. All we need to do now is set the loss argument in our .compile() function to our function name, sign_penalty. We'll examine the training loss again to make sure it's flattened out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model_2 = Sequential()\n",
    "model_2.add(Dense(100, input_dim=scaled_train_features.shape[1], activation='relu'))\n",
    "model_2.add(Dense(20, activation='relu'))\n",
    "model_2.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Fit the model with our custom 'sign_penalty' loss function\n",
    "model_2.compile(optimizer='adam', loss=sign_penalty)\n",
    "history = model_2.fit(scaled_train_features, train_targets, epochs=25)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('loss:' + str(round(history.history['loss'][-1], 6)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've fit our model with the custom loss function, and it's time to see how it is performing. We'll check the R2 values again with sklearn's r2_score() function, and we'll create a scatter plot of predictions versus actual values with plt.scatter(). This will yield some interesting results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate R^2 scores\n",
    "train_preds = model_2.predict(scaled_train_features)\n",
    "test_preds = model_2.predict(scaled_test_features)\n",
    "print(r2_score(train_targets, train_preds))\n",
    "print(r2_score(test_targets, test_preds))\n",
    "\n",
    "# Scatter the predictions vs actual -- this one is interesting!\n",
    "plt.scatter(train_preds, train_targets, label='train')\n",
    "plt.scatter(test_preds, test_targets, label='test')  # plot test set\n",
    "plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combatting overfitting with dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common problem with neural networks is they tend to overfit to training data. What this means is the scoring metric, like R2 or accuracy, is high for the training set, but low for testing and validation sets, and the model is fitting to noise in the training data.\n",
    "\n",
    "We can work towards preventing overfitting by using dropout. This randomly drops some neurons during the training phase, which helps prevent the net from fitting noise in the training data. keras has a Dropout layer that we can use to accomplish this. We need to set the dropout rate, or fraction of connections dropped during training time. This is set as a decimal between 0 and 1 in the Dropout() layer.\n",
    "\n",
    "We're going to go back to the mean squared error loss function for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "# Create model with dropout\n",
    "model_3 = Sequential()\n",
    "model_3.add(Dense(100, input_dim=scaled_train_features.shape[1], activation='relu'))\n",
    "model_3.add(Dropout(0.2))\n",
    "model_3.add(Dense(20, activation='relu'))\n",
    "model_3.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Fit model with mean squared error loss function\n",
    "model_3.compile(optimizer='adam', loss='mse')\n",
    "history = model_3.fit(scaled_train_features, train_targets, epochs=25)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('loss:' + str(round(history.history['loss'][-1], 6)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One approach to improve predictions from machine learning models is ensembling. A basic approach is to average the predictions from multiple models. A more complex approach is to feed predictions of models into another model, which makes final predictions. Both approaches usually improve our overall performance (as long as our individual models are good). If you remember, random forests are also using ensembling of many decision trees.\n",
    "\n",
    "To ensemble our neural net predictions, we'll make predictions with the 3 models we just created -- the basic model, the model with the custom loss function, and the model with dropout. Then we'll combine the predictions with numpy's .hstack() function, and average them across rows with np.mean(predictions, axis=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions from the 3 neural net models\n",
    "train_pred1 = model_1.predict(scaled_train_features)\n",
    "test_pred1 = model_1.predict(scaled_test_features)\n",
    "\n",
    "train_pred2 = model_2.predict(scaled_train_features)\n",
    "test_pred2 = model_2.predict(scaled_test_features)\n",
    "\n",
    "train_pred3 = model_3.predict(scaled_train_features)\n",
    "test_pred3 = model_3.predict(scaled_test_features)\n",
    "\n",
    "# Horizontally stack predictions and take the average across rows\n",
    "train_preds = np.mean(np.hstack((train_pred1, train_pred2, train_pred3)), axis=1)\n",
    "test_preds = np.mean(np.hstack((test_pred1, test_pred2, test_pred3)), axis=1)\n",
    "print(test_preds[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See how the ensemble performed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check performance of our ensembled model to see how it's doing. We should see roughly an average of the R2 scores, as well as a scatter plot that is a mix of our previous models' predictions. The bow-tie shape from the custom loss function model should still be a bit visible, but the edges near x=0 should be softer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Evaluate the R^2 scores\n",
    "print(r2_score(train_targets, train_preds))\n",
    "print(r2_score(test_targets, test_preds))\n",
    "\n",
    "# Scatter the predictions vs actual -- this one is interesting!\n",
    "plt.scatter(train_preds, train_targets, label='train')\n",
    "plt.scatter(test_preds, test_targets, label='test')\n",
    "plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
