{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataCamp - Machine Learning for Finance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore data with some EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's explore the data. Any time we begin a machine learning (ML) project, we need to first do some exploratory data analysis (EDA) to familiarize ourselves with the data. This includes things like:\n",
    "\n",
    "raw data plots\n",
    "histograms\n",
    "and more...\n",
    "I typically begin with raw data plots and histograms. This allows us to understand our data's distributions. If it's a normal distribution, we can use things like parametric statistics.\n",
    "\n",
    "There are two stocks loaded for you into pandas DataFrames: lng_df and spy_df (LNG and SPY). Take a look at them with .head(). We'll use the closing prices and eventually volume as inputs to ML algorithms.\n",
    "\n",
    "Note: We'll call plt.clf() each time we want to make a new plot, or f = plt.figure()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (<ipython-input-1-8e79ebd0bd76>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-8e79ebd0bd76>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    lng_df[____].plot(label=____, ____, secondary_y=True)\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "print(lng_df.head())  # examine the DataFrames\n",
    "print(spy_df.head())  # examine the SPY DataFrame\n",
    "\n",
    "# Plot the Adj_Close columns for SPY and LNG\n",
    "spy_df['Adj_Close'].plot(label='SPY', legend=True)\n",
    "lng_df['Adj_Close'].plot(label='LNG', secondary_y=True)\n",
    "plt.show()  # show the plot\n",
    "plt.clf()  # clear the plot space\n",
    "\n",
    "# Histogram of the daily price change percent of Adj_Close for LNG\n",
    "lng_df['Adj_Close'].pct_change(1).plot.hist(bins=50)\n",
    "plt.xlabel('adjusted close 1-day percent change')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlations are nice to check out before building machine learning models, because we can see which features correlate to the target most strongly. Pearson's correlation coefficient is often used, which only detects linear relationships. It's commonly assumed our data is normally distributed, which we can \"eyeball\" from histograms. Highly correlated variables have a Pearson correlation coefficient near 1 (positively correlated) or -1 (negatively correlated). A value near 0 means the two variables are not linearly correlated.\n",
    "\n",
    "If we use the same time periods for previous price changes and future price changes, we can see if the stock price is mean-reverting (bounces around) or trend-following (goes up if it has been going up recently)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 5-day % changes of Adj_Close for the current day, and 5 days in the future\n",
    "lng_df['5d_future_close'] = lng_df['Adj_Close'].shift(-5)\n",
    "lng_df['5d_close_future_pct'] = lng_df['5d_future_close'].pct_change(5)\n",
    "lng_df['5d_close_pct'] = lng_df['Adj_Close'].pct_change(5)\n",
    "\n",
    "# Calculate the correlation matrix between the 5d close pecentage changes (current and future)\n",
    "corr = lng_df[['5d_close_pct', '5d_close_future_pct']].corr()\n",
    "print(corr)\n",
    "\n",
    "# Scatter the current 5-day percent change vs the future 5-day percent change\n",
    "plt.scatter(lng_df['5d_close_pct'], lng_df['5d_close_future_pct'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create moving average and RSI features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to add historical data to our machine learning models to make better predictions, but adding lots of historical time steps is tricky. Instead, we can condense information from previous points into a single timestep with indicators.\n",
    "\n",
    "A moving average is one of the simplest indicators - it's the average of previous data points. This is the function talib.SMA() from the TAlib library.\n",
    "\n",
    "Another common technical indicator is the relative strength index (RSI). This is defined by:\n",
    "\n",
    "RSI=100âˆ’1001+RS \n",
    "\n",
    "RS=average gain over n periods / average loss over n periods\n",
    "\n",
    "The n periods is set in talib.RSI() as the timeperiod argument.\n",
    "\n",
    "A common period for RSI is 14, so we'll use that as one setting in our calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['5d_close_pct']  # a list of the feature names for later\n",
    "\n",
    "# Create moving averages and rsi for timeperiods of 14, 30, 50, and 200\n",
    "for n in [14, 30, 50, 200]:\n",
    "\n",
    "    # Create the moving average indicator and divide by Adj_Close\n",
    "    lng_df['ma' + str(n)] = talib.SMA(lng_df['Adj_Close'].values,\n",
    "                              timeperiod=n) / lng_df['Adj_Close']\n",
    "    # Create the RSI indicator\n",
    "    lng_df['rsi' + str(n)] = talib.RSI(lng_df['Adj_Close'].values, timeperiod=n)\n",
    "    \n",
    "    # Add rsi and moving average to the feature name list\n",
    "    feature_names = feature_names + ['ma' + str(n), 'rsi' + str(n)]\n",
    "\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create features and targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We almost have features and targets that are machine-learning ready -- we have features from current price changes (5d_close_pct) and indicators (moving averages and RSI), and we created targets of future price changes (5d_close_future_pct). Now we need to break these up into separate numpy arrays so we can feed them into machine learning algorithms.\n",
    "\n",
    "Our indicators also cause us to have missing values at the beginning of the DataFrame due to the calculations. We could backfill this data, fill it with a single value, or drop the rows. Dropping the rows is a good choice, so our machine learning algorithms aren't confused by any sort of backfilled or 0-filled data. Pandas has a .dropna() function which we will use to drop any rows with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all na values\n",
    "lng_df = lng_df.dropna()\n",
    "\n",
    "# Create features and targets\n",
    "# use feature_names for features; 5d_close_future_pct for targets\n",
    "features = lng_df[feature_names]\n",
    "targets = lng_df['5d_close_future_pct']\n",
    "\n",
    "# Create DataFrame from target column and feature columns\n",
    "feat_targ_df = lng_df[['5d_close_future_pct'] + feature_names]\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr = feat_targ_df.corr()\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we fit our first machine learning model, let's look at the correlations between features and targets. Ideally we want large (near 1 or -1) correlations between features and targets. Examining correlations can help us tweak features to maximize correlation (for example, altering the timeperiod argument in the talib functions). It can also help us remove features that aren't correlated to the target.\n",
    "\n",
    "To easily plot a correlation matrix, we can use seaborn's heatmap() function. This takes a correlation matrix as the first argument, and has many other options. Check out the annot option -- this will help us turn on annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot heatmap of correlation matrix\n",
    "sns.heatmap(corr, annot=True)\n",
    "plt.yticks(rotation=0); plt.xticks(rotation=90)  # fix ticklabel directions\n",
    "plt.tight_layout()  # fits plot area to the plot, \"tightly\"\n",
    "plt.show()  # show the plot\n",
    "plt.clf() # clear the plot area\n",
    "\n",
    "# Create a scatter plot of the most highly correlated variable with the target\n",
    "plt.scatter(x=lng_df['ma200'], y=lng_df['5d_close_future_pct'] )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train and test features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we fit our linear model, we want to add a constant to our features, so we have an intercept for our linear model.\n",
    "\n",
    "We also want to create train and test features. This is so we can fit our model to the train dataset, and evaluate performance on the test dataset. We always want to check performance on data the model has not seen to make sure we're not overfitting, which is memorizing patterns in the training data too exactly.\n",
    "\n",
    "With a time series like this, we typically want to use the oldest data as our training set, and the newest data as our test set. This is so we can evaluate the performance of the model on the most recent data, which will more realistically simulate predictions on data we haven't seen yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the statsmodels.api library with the alias sm\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Add a constant to the features\n",
    "linear_features = sm.add_constant(features)\n",
    "\n",
    "# Create a size for the training set that is 85% of the total number of samples\n",
    "train_size = int(0.85 * targets.shape[0])\n",
    "train_features = linear_features[:train_size]\n",
    "train_targets = targets[:train_size]\n",
    "test_features = linear_features[train_size:]\n",
    "test_targets = targets[train_size:]\n",
    "print(linear_features.shape, train_features.shape, test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now fit a linear model, because they are simple and easy to understand. Once we've fit our model, we can see which predictor variables appear to be meaningfully linearly correlated with the target, as well as their magnitude of effect on the target. Our judgment of whether or not predictors are significant is based on the p-values of coefficients. This is using a t-test to statistically test if the coefficient significantly differs from 0. The p-value is the percent chance that the coefficient for a feature does not differ from zero. Typically, we take a p-value of less than 0.05 to mean the coefficient is significantly different from 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the linear model and complete the least squares fit\n",
    "model = sm.OLS(train_targets, train_features)\n",
    "results = model.fit()  # fit the model\n",
    "print(results.summary())\n",
    "\n",
    "# examine pvalues\n",
    "# Features with p <= 0.05 are typically considered significantly different from 0\n",
    "print(results.pvalues)\n",
    "\n",
    "# Make predictions from our model for train and test sets\n",
    "train_predictions = results.predict(train_features)\n",
    "test_predictions = results.predict(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate our results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our linear fit and predictions, we want to see how good the predictions are so we can decide if our model is any good or not. Ideally, we want to back-test any type of trading strategy. However, this is a complex and typically time-consuming experience.\n",
    "\n",
    "A quicker way to understand the performance of our model is looking at regression evaluation metrics like R2, and plotting the predictions versus the actual values of the targets. Perfect predictions would form a straight, diagonal line in such a plot, making it easy for us to eyeball how our predictions are doing in different regions of price changes. We can use matplotlib's .scatter() function to create scatter plots of the predictions and actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter the predictions vs the targets with 80% transparency\n",
    "plt.scatter(train_predictions, train_targets, alpha=0.2, color='b', label='train')\n",
    "plt.scatter(test_predictions, test_targets, alpha=0.2, color='r', label='test')\n",
    "\n",
    "# Plot the perfect prediction line\n",
    "xmin, xmax = plt.xlim()\n",
    "plt.plot(np.arange(xmin, xmax, 0.01), np.arange(xmin, xmax, 0.01), c='k')\n",
    "\n",
    "# Set the axis labels and show the plot\n",
    "plt.xlabel('predictions')\n",
    "plt.ylabel('actual')\n",
    "plt.legend()  # show the legend\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering from volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use non-linear models to make more accurate predictions. With linear models, features must be linearly correlated to the target. Other machine learning models can combine features in non-linear ways. For example, what if the price goes up when the moving average of price is going up, and the moving average of volume is going down? The only way to capture those interactions is to either multiply the features, or to use a machine learning algorithm that can handle non-linearity (e.g. random forests).\n",
    "\n",
    "To incorporate more information that may interact with other features, we can add in weakly-correlated features. First we will add volume data, which we have in the lng_df as the Adj_Volume column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2 new volume features, 1-day % change and 5-day SMA of the % change\n",
    "new_features = ['Adj_Volume_1d_change', 'Adj_Volume_1d_change_SMA']\n",
    "feature_names.extend(new_features)\n",
    "lng_df['Adj_Volume_1d_change'] = lng_df['Adj_Volume'].pct_change()\n",
    "lng_df['Adj_Volume_1d_change_SMA'] = talib.SMA(lng_df['Adj_Volume_1d_change'].values,timeperiod=5)\n",
    "\n",
    "# Plot histogram of volume % change data\n",
    "lng_df[new_features].plot(kind='hist', sharex=False, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create day-of-week features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can engineer datetime features to add even more information for our non-linear models. Most financial data has datetimes, which have lots of information in them -- year, month, day, and sometimes hour, minute, and second. But we can also get the day of the week, and things like the quarter of the year, or the elapsed time since some event (e.g. earnings reports).\n",
    "\n",
    "We are only going to get the day of the week here, since our dataset doesn't go back very far in time. The dayofweek property from the pandas datetime index will help us get the day of the week. Then we will dummy dayofweek with pandas' get_dummies(). This creates columns for each day of the week with binary values (0 or 1). We drop the first column because it can be inferred from the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas' get_dummies function to get dummies for day of the week\n",
    "days_of_week = pd.get_dummies(lng_df.index.dayofweek,\n",
    "                              prefix='weekday',\n",
    "                              drop_first=True)\n",
    "\n",
    "# Set the index as the original dataframe index for merging\n",
    "days_of_week.index = lng_df.index\n",
    "\n",
    "# Join the dataframe with the days of week dataframe\n",
    "lng_df = pd.concat([lng_df, days_of_week], axis=1)\n",
    "\n",
    "# Add days of week to feature names\n",
    "feature_names.extend(['weekday_' + str(i) for i in range(1, 5)])\n",
    "lng_df.dropna(inplace=True)  # drop missing values in-place\n",
    "print(lng_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine correlations of the new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our volume and datetime features, we want to check the correlations between our new features (stored in the new_features list) and the target (5d_close_future_pct) to see how strongly they are related. Recall pandas has the built-in .corr() method for DataFrames, and seaborn has a nice heatmap() function to show the correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the weekday labels to the new_features list\n",
    "new_features.extend(['weekday_' + str(i) for i in range(1, 5)])\n",
    "\n",
    "# Plot the correlations between the new features and the targets\n",
    "sns.heatmap(lng_df[new_features + ['5d_close_future_pct']].corr(), annot=True)\n",
    "plt.yticks(rotation=0)  # ensure y-axis ticklabels are horizontal\n",
    "plt.xticks(rotation=90)  # ensure x-axis ticklabels are vertical\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
