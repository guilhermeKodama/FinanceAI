{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataCamp - Machine Learning for Finance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore data with some EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's explore the data. Any time we begin a machine learning (ML) project, we need to first do some exploratory data analysis (EDA) to familiarize ourselves with the data. This includes things like:\n",
    "\n",
    "raw data plots\n",
    "histograms\n",
    "and more...\n",
    "I typically begin with raw data plots and histograms. This allows us to understand our data's distributions. If it's a normal distribution, we can use things like parametric statistics.\n",
    "\n",
    "There are two stocks loaded for you into pandas DataFrames: lng_df and spy_df (LNG and SPY). Take a look at them with .head(). We'll use the closing prices and eventually volume as inputs to ML algorithms.\n",
    "\n",
    "Note: We'll call plt.clf() each time we want to make a new plot, or f = plt.figure()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (<ipython-input-1-8e79ebd0bd76>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-8e79ebd0bd76>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    lng_df[____].plot(label=____, ____, secondary_y=True)\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "print(lng_df.head())  # examine the DataFrames\n",
    "print(spy_df.head())  # examine the SPY DataFrame\n",
    "\n",
    "# Plot the Adj_Close columns for SPY and LNG\n",
    "spy_df['Adj_Close'].plot(label='SPY', legend=True)\n",
    "lng_df['Adj_Close'].plot(label='LNG', secondary_y=True)\n",
    "plt.show()  # show the plot\n",
    "plt.clf()  # clear the plot space\n",
    "\n",
    "# Histogram of the daily price change percent of Adj_Close for LNG\n",
    "lng_df['Adj_Close'].pct_change(1).plot.hist(bins=50)\n",
    "plt.xlabel('adjusted close 1-day percent change')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlations are nice to check out before building machine learning models, because we can see which features correlate to the target most strongly. Pearson's correlation coefficient is often used, which only detects linear relationships. It's commonly assumed our data is normally distributed, which we can \"eyeball\" from histograms. Highly correlated variables have a Pearson correlation coefficient near 1 (positively correlated) or -1 (negatively correlated). A value near 0 means the two variables are not linearly correlated.\n",
    "\n",
    "If we use the same time periods for previous price changes and future price changes, we can see if the stock price is mean-reverting (bounces around) or trend-following (goes up if it has been going up recently)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 5-day % changes of Adj_Close for the current day, and 5 days in the future\n",
    "lng_df['5d_future_close'] = lng_df['Adj_Close'].shift(-5)\n",
    "lng_df['5d_close_future_pct'] = lng_df['5d_future_close'].pct_change(5)\n",
    "lng_df['5d_close_pct'] = lng_df['Adj_Close'].pct_change(5)\n",
    "\n",
    "# Calculate the correlation matrix between the 5d close pecentage changes (current and future)\n",
    "corr = lng_df[['5d_close_pct', '5d_close_future_pct']].corr()\n",
    "print(corr)\n",
    "\n",
    "# Scatter the current 5-day percent change vs the future 5-day percent change\n",
    "plt.scatter(lng_df['5d_close_pct'], lng_df['5d_close_future_pct'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create moving average and RSI features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to add historical data to our machine learning models to make better predictions, but adding lots of historical time steps is tricky. Instead, we can condense information from previous points into a single timestep with indicators.\n",
    "\n",
    "A moving average is one of the simplest indicators - it's the average of previous data points. This is the function talib.SMA() from the TAlib library.\n",
    "\n",
    "Another common technical indicator is the relative strength index (RSI). This is defined by:\n",
    "\n",
    "RSI=100âˆ’1001+RS \n",
    "\n",
    "RS=average gain over n periods / average loss over n periods\n",
    "\n",
    "The n periods is set in talib.RSI() as the timeperiod argument.\n",
    "\n",
    "A common period for RSI is 14, so we'll use that as one setting in our calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['5d_close_pct']  # a list of the feature names for later\n",
    "\n",
    "# Create moving averages and rsi for timeperiods of 14, 30, 50, and 200\n",
    "for n in [14, 30, 50, 200]:\n",
    "\n",
    "    # Create the moving average indicator and divide by Adj_Close\n",
    "    lng_df['ma' + str(n)] = talib.SMA(lng_df['Adj_Close'].values,\n",
    "                              timeperiod=n) / lng_df['Adj_Close']\n",
    "    # Create the RSI indicator\n",
    "    lng_df['rsi' + str(n)] = talib.RSI(lng_df['Adj_Close'].values, timeperiod=n)\n",
    "    \n",
    "    # Add rsi and moving average to the feature name list\n",
    "    feature_names = feature_names + ['ma' + str(n), 'rsi' + str(n)]\n",
    "\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create features and targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We almost have features and targets that are machine-learning ready -- we have features from current price changes (5d_close_pct) and indicators (moving averages and RSI), and we created targets of future price changes (5d_close_future_pct). Now we need to break these up into separate numpy arrays so we can feed them into machine learning algorithms.\n",
    "\n",
    "Our indicators also cause us to have missing values at the beginning of the DataFrame due to the calculations. We could backfill this data, fill it with a single value, or drop the rows. Dropping the rows is a good choice, so our machine learning algorithms aren't confused by any sort of backfilled or 0-filled data. Pandas has a .dropna() function which we will use to drop any rows with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all na values\n",
    "lng_df = lng_df.dropna()\n",
    "\n",
    "# Create features and targets\n",
    "# use feature_names for features; 5d_close_future_pct for targets\n",
    "features = lng_df[feature_names]\n",
    "targets = lng_df['5d_close_future_pct']\n",
    "\n",
    "# Create DataFrame from target column and feature columns\n",
    "feat_targ_df = lng_df[['5d_close_future_pct'] + feature_names]\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr = feat_targ_df.corr()\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we fit our first machine learning model, let's look at the correlations between features and targets. Ideally we want large (near 1 or -1) correlations between features and targets. Examining correlations can help us tweak features to maximize correlation (for example, altering the timeperiod argument in the talib functions). It can also help us remove features that aren't correlated to the target.\n",
    "\n",
    "To easily plot a correlation matrix, we can use seaborn's heatmap() function. This takes a correlation matrix as the first argument, and has many other options. Check out the annot option -- this will help us turn on annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot heatmap of correlation matrix\n",
    "sns.heatmap(corr, annot=True)\n",
    "plt.yticks(rotation=0); plt.xticks(rotation=90)  # fix ticklabel directions\n",
    "plt.tight_layout()  # fits plot area to the plot, \"tightly\"\n",
    "plt.show()  # show the plot\n",
    "plt.clf() # clear the plot area\n",
    "\n",
    "# Create a scatter plot of the most highly correlated variable with the target\n",
    "plt.scatter(x=lng_df['ma200'], y=lng_df['5d_close_future_pct'] )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train and test features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we fit our linear model, we want to add a constant to our features, so we have an intercept for our linear model.\n",
    "\n",
    "We also want to create train and test features. This is so we can fit our model to the train dataset, and evaluate performance on the test dataset. We always want to check performance on data the model has not seen to make sure we're not overfitting, which is memorizing patterns in the training data too exactly.\n",
    "\n",
    "With a time series like this, we typically want to use the oldest data as our training set, and the newest data as our test set. This is so we can evaluate the performance of the model on the most recent data, which will more realistically simulate predictions on data we haven't seen yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the statsmodels.api library with the alias sm\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Add a constant to the features\n",
    "linear_features = sm.add_constant(features)\n",
    "\n",
    "# Create a size for the training set that is 85% of the total number of samples\n",
    "train_size = int(0.85 * targets.shape[0])\n",
    "train_features = linear_features[:train_size]\n",
    "train_targets = targets[:train_size]\n",
    "test_features = linear_features[train_size:]\n",
    "test_targets = targets[train_size:]\n",
    "print(linear_features.shape, train_features.shape, test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now fit a linear model, because they are simple and easy to understand. Once we've fit our model, we can see which predictor variables appear to be meaningfully linearly correlated with the target, as well as their magnitude of effect on the target. Our judgment of whether or not predictors are significant is based on the p-values of coefficients. This is using a t-test to statistically test if the coefficient significantly differs from 0. The p-value is the percent chance that the coefficient for a feature does not differ from zero. Typically, we take a p-value of less than 0.05 to mean the coefficient is significantly different from 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the linear model and complete the least squares fit\n",
    "model = sm.OLS(train_targets, train_features)\n",
    "results = model.fit()  # fit the model\n",
    "print(results.summary())\n",
    "\n",
    "# examine pvalues\n",
    "# Features with p <= 0.05 are typically considered significantly different from 0\n",
    "print(results.pvalues)\n",
    "\n",
    "# Make predictions from our model for train and test sets\n",
    "train_predictions = results.predict(train_features)\n",
    "test_predictions = results.predict(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate our results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our linear fit and predictions, we want to see how good the predictions are so we can decide if our model is any good or not. Ideally, we want to back-test any type of trading strategy. However, this is a complex and typically time-consuming experience.\n",
    "\n",
    "A quicker way to understand the performance of our model is looking at regression evaluation metrics like R2, and plotting the predictions versus the actual values of the targets. Perfect predictions would form a straight, diagonal line in such a plot, making it easy for us to eyeball how our predictions are doing in different regions of price changes. We can use matplotlib's .scatter() function to create scatter plots of the predictions and actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter the predictions vs the targets with 80% transparency\n",
    "plt.scatter(train_predictions, train_targets, alpha=0.2, color='b', label='train')\n",
    "plt.scatter(test_predictions, test_targets, alpha=0.2, color='r', label='test')\n",
    "\n",
    "# Plot the perfect prediction line\n",
    "xmin, xmax = plt.xlim()\n",
    "plt.plot(np.arange(xmin, xmax, 0.01), np.arange(xmin, xmax, 0.01), c='k')\n",
    "\n",
    "# Set the axis labels and show the plot\n",
    "plt.xlabel('predictions')\n",
    "plt.ylabel('actual')\n",
    "plt.legend()  # show the legend\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
